{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from typing import List, Tuple\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
    "\n",
    "import accelerate\n",
    "\n",
    "from pathlib import Path\n",
    "root_dir = Path().resolve()\n",
    "\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "from omnigen2.pipelines.omnigen2.pipeline_omnigen2 import OmniGen2Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_collage(images: List[torch.Tensor]) -> Image.Image:\n",
    "    \"\"\"Create a horizontal collage from a list of images.\"\"\"\n",
    "    max_height = max(img.shape[-2] for img in images)\n",
    "    total_width = sum(img.shape[-1] for img in images)\n",
    "    canvas = torch.zeros((3, max_height, total_width), device=images[0].device)\n",
    "    \n",
    "    current_x = 0\n",
    "    for img in images:\n",
    "        h, w = img.shape[-2:]\n",
    "        canvas[:, :h, current_x:current_x+w] = img * 0.5 + 0.5\n",
    "        current_x += w\n",
    "    \n",
    "    return to_pil_image(canvas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_image_path: List[str] = []) -> Tuple[str, str, List[Image.Image]]:\n",
    "    \"\"\"Preprocess the input images.\"\"\"\n",
    "    # Process input images\n",
    "    input_images = []\n",
    "\n",
    "    if input_image_path:\n",
    "        if isinstance(input_image_path, str):\n",
    "            input_image_path = [input_image_path]\n",
    "            \n",
    "        if len(input_image_path) == 1 and os.path.isdir(input_image_path[0]):\n",
    "            input_images = [Image.open(os.path.join(input_image_path[0], f)) \n",
    "                          for f in os.listdir(input_image_path[0])]\n",
    "        else:\n",
    "            input_images = [Image.open(path) for path in input_image_path]\n",
    "\n",
    "    return input_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = accelerate.Accelerator()\n",
    "pipeline = OmniGen2Pipeline.from_pretrained(\"/share_2/luoxin/projects/OmniGen2/pretrained_models/omnigen2_pipe\",\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            trust_remote_code=True,\n",
    "                                            token=\"REMOVED_TOKENYVrtMysWgKpjKpdiquPiOMevDqhiDYkKRL\")\n",
    "pipeline = pipeline.to(accelerator.device, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text to image generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=accelerator.device).manual_seed(223)\n",
    "\n",
    "instruction = \"A dog running in the park\"\n",
    "negative_prompt = \"\"\n",
    "\n",
    "results = pipeline(\n",
    "    prompt=instruction,\n",
    "    input_images=[],\n",
    "    width=1024,\n",
    "    height=1024,\n",
    "    num_inference_steps=28,\n",
    "    max_sequence_length=1024,\n",
    "    text_guidance_scale=5.0,\n",
    "    image_guidance_scale=1.0,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_images_per_prompt=3,\n",
    "    generator=generator,\n",
    "    output_type=\"pil\",\n",
    ")\n",
    "\n",
    "vis_images = [to_tensor(image) * 2 - 1 for image in results.images]\n",
    "output_image = create_collage(vis_images)\n",
    "\n",
    "display(output_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Editing with instruction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=accelerator.device).manual_seed(223)\n",
    "\n",
    "instruction = \"Add a beautiful girl with long flowing hair seated beside the teddy bear on the park bench.\"\n",
    "negative_prompt = \"\"\n",
    "\n",
    "input_images = preprocess(\"example_images/02.jpg\")\n",
    "\n",
    "results = pipeline(\n",
    "    prompt=instruction,\n",
    "    input_images=input_images,\n",
    "    width=1024,\n",
    "    height=1024,\n",
    "    num_inference_steps=28,\n",
    "    max_sequence_length=1024,\n",
    "    text_guidance_scale=5.0,\n",
    "    image_guidance_scale=1.8,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_images_per_prompt=3,\n",
    "    generator=generator,\n",
    "    output_type=\"pil\",\n",
    ")\n",
    "# !! Uncomment following lines to visualize the input images\n",
    "# vis_images = [to_tensor(image) * 2 - 1 for image in input_images]\n",
    "# input_images = create_collage(vis_images)\n",
    "# display(input_images)\n",
    "\n",
    "vis_images = [to_tensor(image) * 2 - 1 for image in results.images]\n",
    "output_image = create_collage(vis_images)\n",
    "display(output_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subject-Driven Editing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=accelerator.device).manual_seed(223)\n",
    "\n",
    "instruction = \"The car toy and the bear toy are placed on the luxury hotel bed.\"\n",
    "negative_prompt = \"\"\n",
    "\n",
    "input_images = preprocess(\"example_images\")\n",
    "\n",
    "results = pipeline(\n",
    "    prompt=instruction,\n",
    "    input_images=input_images,\n",
    "    width=1024,\n",
    "    height=1024,\n",
    "    num_inference_steps=28,\n",
    "    max_sequence_length=1024,\n",
    "    text_guidance_scale=5.0,\n",
    "    image_guidance_scale=1.8,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_images_per_prompt=3,\n",
    "    generator=generator,\n",
    "    output_type=\"pil\",\n",
    ")\n",
    "\n",
    "# !! Uncomment following lines to visualize the input images\n",
    "# vis_images = [to_tensor(image) * 2 - 1 for image in input_images]\n",
    "# input_images = create_collage(vis_images)\n",
    "# display(input_images)\n",
    "\n",
    "vis_images = [to_tensor(image) * 2 - 1 for image in results.images]\n",
    "output_image = create_collage(vis_images)\n",
    "\n",
    "display(output_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
